import PIL
import os
import torch
import torchvision
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms
from torchvision.transforms import ToTensor

from torchvision import models
num_classes = 17
model = models.inception_v3(pretrained=False)
model.aux_logits = False
model.fc =  nn.Linear(model.fc.in_features, num_classes)
model = model.cuda()

model.load_state_dict(torch.load(r'E:\fbl_pt1-test1.pth'))

train_transform = transforms.Compose([
    transforms.RandomResizedCrop(299),
    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                   std=[0.229, 0.224, 0.225])
])

train_data = datasets.ImageFolder(r'E:\train_images', train_transform)
batch_size = 16
train_loader = DataLoader(train_data,batch_size=batch_size, shuffle=True)


loss = nn.CrossEntropyLoss()
optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

num_epochs = 55

for epoch in range(num_epochs):
    total_batch = len(train_data)//batch_size
    for i, (batch_images, batch_labels) in enumerate(train_loader):
        # Zero your gradients for every batch!
        optimizer.zero_grad()
        inputs = batch_images.cuda()
        labels = batch_labels.cuda()
        # Make predictions for this batch
        outputs  = model(inputs)
        
        # Compute the loss and its gradients
        cost = loss(outputs , labels)
        cost.backward()
        # Adjust learning weights
        optimizer.step()

        if (i+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], lter [{i+1}/{total_batch}] Loss: {cost.item():.4f}')

torch.save(model.state_dict(), 'fbl_pt1-test3.pth')            
